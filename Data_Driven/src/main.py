"""
ğŸš€ DOLIBARR DATA-DRIVEN SYSTEM - Point d'entrÃ©e principal
Architecture: DataLake -> DataWarehouse -> Smart Analytics
"""

import click
from datetime import datetime
from loguru import logger
import sys
import os

# Configuration des logs
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)
logger.add(
    "logs/dolibarr_data_driven_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="30 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG"
)

# Imports services
from services.dolibarr_connector import DolibarrConnector
from services.data_lake import DataLake
from services.data_warehouse import DataWarehouse
from services.smart_analytics import SmartAnalytics

@click.group()
@click.version_option(version="1.0.0")
def cli():
    """ğŸ—ï¸ SystÃ¨me Data-Driven Dolibarr BTP
    
    Architecture complÃ¨te: DataLake â†’ DataWarehouse â†’ Smart Analytics
    """
    logger.info("ğŸš€ Dolibarr Data-Driven System v1.0.0")

@cli.command()
@click.option('--test-connection', is_flag=True, help='Test uniquement la connexion')
def extract(test_connection):
    """ğŸ“¥ Extraction des donnÃ©es depuis Dolibarr vers DataLake"""
    logger.info("ğŸ”„ DÃ©marrage extraction DataLake...")
    
    try:
        # Test connexion si demandÃ©
        if test_connection:
            connector = DolibarrConnector()
            if connector.test_connection():
                logger.success("âœ… Connexion Dolibarr OK")
                return
            else:
                logger.error("âŒ Ã‰chec connexion Dolibarr")
                sys.exit(1)
        
        # Extraction complÃ¨te
        data_lake = DataLake()
        extraction_info = data_lake.extract_and_store_daily()
        
        # Rapport extraction
        total_records = sum(d.get('count', 0) for d in extraction_info.get('datasets', {}).values())
        logger.success(f"âœ… Extraction terminÃ©e: {total_records} enregistrements")
        
        # Rapport qualitÃ©
        quality_report = data_lake.generate_quality_report()
        logger.info(f"ğŸ“Š Score qualitÃ© global: {quality_report.get('overall_quality_score', 0)}/100")
        
    except Exception as e:
        logger.error(f"âŒ Erreur extraction: {e}")
        sys.exit(1)

@cli.command()
@click.option('--full-rebuild', is_flag=True, help='Reconstruction complÃ¨te du warehouse')
def warehouse(full_rebuild):
    """ğŸ¢ Chargement DataWarehouse depuis DataLake"""
    logger.info("ğŸ”„ DÃ©marrage chargement DataWarehouse...")
    
    try:
        dw = DataWarehouse()
        
        if full_rebuild:
            logger.info("ğŸ”„ Reconstruction complÃ¨te du DataWarehouse...")
            # Ici on pourrait ajouter la logique de drop/recreate tables
        
        # ETL complet
        stats = dw.full_etl_process()
        
        # CrÃ©ation des vues analytiques
        dw.create_analytical_views()
        
        logger.success(f"âœ… DataWarehouse chargÃ©: {stats}")
        
    except Exception as e:
        logger.error(f"âŒ Erreur DataWarehouse: {e}")
        sys.exit(1)

@cli.command()
@click.option('--module', type=click.Choice(['anomalies', 'segmentation', 'prediction', 'insights', 'all']), 
              default='all', help='Module d\'analyse Ã  exÃ©cuter')
def analytics(module):
    """ğŸ§  ExÃ©cution Smart Analytics et Machine Learning"""
    logger.info(f"ğŸ”„ DÃ©marrage Smart Analytics - Module: {module}")
    
    try:
        smart_analytics = SmartAnalytics()
        
        if module == 'all':
            # Analyse complÃ¨te
            results = smart_analytics.run_full_analysis()
            logger.info("ğŸ“Š RÃ©sultats analyse complÃ¨te:")
            
            for module_name, module_results in results.get('modules', {}).items():
                logger.info(f"  â€¢ {module_name}: {len(module_results.get('anomalies', module_results.get('segments', module_results.get('predictions', module_results.get('insights', [])))))} rÃ©sultats")
        
        elif module == 'anomalies':
            results = smart_analytics.detect_anomalies_interventions()
            logger.info(f"ğŸ” DÃ©tection anomalies: {len(results.get('anomalies', []))} anomalies dÃ©tectÃ©es")
        
        elif module == 'segmentation':
            results = smart_analytics.customer_segmentation()
            logger.info(f"ğŸ‘¥ Segmentation: {len(results.get('segments', []))} segments crÃ©Ã©s")
        
        elif module == 'prediction':
            results = smart_analytics.predict_stock_needs()
            logger.info(f"ğŸ“¦ PrÃ©diction stock: {len(results.get('predictions', []))} produits analysÃ©s")
        
        elif module == 'insights':
            results = smart_analytics.generate_performance_insights()
            logger.info(f"ğŸ“Š Insights: {len(results.get('insights', []))} insights gÃ©nÃ©rÃ©s")
        
        logger.success("âœ… Analyse terminÃ©e")
        
    except Exception as e:
        logger.error(f"âŒ Erreur Smart Analytics: {e}")
        sys.exit(1)

@cli.command()
@click.option('--days', default=7, help='PÃ©riode d\'analyse en jours')
def quality(days):
    """ğŸ“‹ Rapport qualitÃ© des donnÃ©es"""
    logger.info(f"ğŸ“Š GÃ©nÃ©ration rapport qualitÃ© ({days} jours)...")
    
    try:
        data_lake = DataLake()
        report = data_lake.generate_quality_report(period_days=days)
        
        logger.info("ğŸ“Š Rapport QualitÃ©:")
        logger.info(f"  â€¢ Datasets analysÃ©s: {report.get('datasets_analyzed', 0)}")
        logger.info(f"  â€¢ Total enregistrements: {report.get('total_records', 0):,}")
        logger.info(f"  â€¢ Score qualitÃ© global: {report.get('overall_quality_score', 0)}/100")
        logger.info(f"  â€¢ ProblÃ¨mes dÃ©tectÃ©s: {len(report.get('issues_found', []))}")
        
        # DÃ©tail des problÃ¨mes
        for issue in report.get('issues_found', []):
            logger.warning(f"    âš ï¸ {issue['dataset']}: {issue['issue']} (score: {issue['score']})")
        
        logger.success("âœ… Rapport qualitÃ© gÃ©nÃ©rÃ©")
        
    except Exception as e:
        logger.error(f"âŒ Erreur rapport qualitÃ©: {e}")
        sys.exit(1)

@cli.command()
def pipeline():
    """ğŸ”„ Pipeline complet: Extract â†’ Warehouse â†’ Analytics"""
    logger.info("ğŸš€ DÃ©marrage pipeline complet Data-Driven...")
    
    start_time = datetime.now()
    
    try:
        # 1. Extraction DataLake
        logger.info("1ï¸âƒ£ Extraction vers DataLake...")
        data_lake = DataLake()
        extraction_info = data_lake.extract_and_store_daily()
        
        # 2. Chargement DataWarehouse
        logger.info("2ï¸âƒ£ Chargement DataWarehouse...")
        dw = DataWarehouse()
        warehouse_stats = dw.full_etl_process()
        
        # 3. Analytics & ML
        logger.info("3ï¸âƒ£ Smart Analytics...")
        smart_analytics = SmartAnalytics()
        analytics_results = smart_analytics.run_full_analysis()
        
        # 4. Rapport final
        duration = datetime.now() - start_time
        
        logger.success("âœ… Pipeline complet terminÃ©!")
        logger.info(f"â±ï¸ DurÃ©e totale: {duration.total_seconds():.1f}s")
        logger.info("ğŸ“Š RÃ©sumÃ©:")
        
        # Extraction
        total_extracted = sum(d.get('count', 0) for d in extraction_info.get('datasets', {}).values())
        logger.info(f"  â€¢ Extraction: {total_extracted} enregistrements")
        
        # Warehouse
        total_warehouse = sum(warehouse_stats.values()) if warehouse_stats else 0
        logger.info(f"  â€¢ DataWarehouse: {total_warehouse} enregistrements")
        
        # Analytics
        total_insights = len(analytics_results.get('modules', {}).get('performance_insights', {}).get('insights', []))
        logger.info(f"  â€¢ Smart Analytics: {total_insights} insights gÃ©nÃ©rÃ©s")
        
    except Exception as e:
        logger.error(f"âŒ Erreur pipeline: {e}")
        sys.exit(1)

@cli.command()
@click.option('--days', default=90, help='Jours de donnÃ©es Ã  conserver')
def cleanup(days):
    """ğŸ—‘ï¸ Nettoyage des anciennes donnÃ©es"""
    logger.info(f"ğŸ—‘ï¸ Nettoyage donnÃ©es > {days} jours...")
    
    try:
        data_lake = DataLake()
        deleted_files = data_lake.cleanup_old_data(days_to_keep=days)
        
        logger.success(f"âœ… Nettoyage terminÃ©: {deleted_files} fichiers supprimÃ©s")
        
    except Exception as e:
        logger.error(f"âŒ Erreur nettoyage: {e}")
        sys.exit(1)

@cli.command()
def status():
    """ğŸ“Š Statut global du systÃ¨me"""
    logger.info("ğŸ“Š VÃ©rification statut systÃ¨me...")
    
    try:
        # Test connexion Dolibarr
        connector = DolibarrConnector()
        dolibarr_ok = connector.test_connection()
        
        # Statut DataLake
        data_lake = DataLake()
        datasets = data_lake.list_datasets()
        total_datasets = sum(len(files) for files in datasets.values())
        
        # Statut DataWarehouse
        dw = DataWarehouse()
        warehouse_stats = dw.get_warehouse_stats()
        
        logger.info("ğŸ—ï¸ Statut SystÃ¨me Data-Driven:")
        logger.info(f"  â€¢ Connexion Dolibarr: {'âœ… OK' if dolibarr_ok else 'âŒ ERREUR'}")
        logger.info(f"  â€¢ DataLake: {total_datasets} datasets disponibles")
        logger.info(f"  â€¢ DataWarehouse: {sum(warehouse_stats.values()) if warehouse_stats else 0} enregistrements")
        
        # DÃ©tail par catÃ©gorie
        logger.info("ğŸ“ Datasets DataLake:")
        for category, files in datasets.items():
            logger.info(f"    â€¢ {category}: {len(files)} fichiers")
        
        logger.info("ğŸ¢ Tables DataWarehouse:")
        for table, count in warehouse_stats.items():
            logger.info(f"    â€¢ {table}: {count:,} enregistrements")
        
    except Exception as e:
        logger.error(f"âŒ Erreur statut: {e}")
        sys.exit(1)

if __name__ == '__main__':
    # CrÃ©er rÃ©pertoire logs
    os.makedirs('logs', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('data/lake', exist_ok=True)
    
    cli()